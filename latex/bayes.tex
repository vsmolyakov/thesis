This section focuses on derivation of Bayesian inference algorithms for a variety of graphical models. The purpose of this section to practice derivation and implementation of Bayesian inference algorithms focusing on Markov-Chain Monte-Carlo (MCMC) sampling techniques from high dimensional posterior distributions. 

%TODO: note on markov chain techniques (why markov dependency between samples at all)

\subsection{MAP / MLE}
\subsubsection{Naive Bayes}

\subsection{Expectation Maximization (EM)}

The EM algorithm provides a way of computing ML/MAP estimates when we have unobserved latent variables and/or missing data. EM exploits the fact that if the data were fully observed, then the ML/MAP estimates would be easy to compute. In particular, EM is an iterative algorithm which alternates between inferring the missing values given the parameters (E step) and then optimizing the parameters given filled in data (M step).\\ 

In the EM algorithm, we define the complete data log-likelihood $l_c(\theta)$ where $x_i$ are the observed random variables and $z_i$ are unobserved. Since we don't know $z_i$ we can't compute $p(x_i, z_i|\theta)$ but we can compute an expectation of $l_c(\theta)$ wrt to parameters $\theta^{(k-1)}$ from the previous iteration.  
\begin{equation}
    l_c(\theta) = \sum_{i=1}^{N}\log p(x_i, z_i|\theta) 
\end{equation}
The goal of the E-step is to compute $Q(\theta, \theta^{(k-1)})$ on which the ML/MAP estimates depend. The goal of the M-step is to re-compute $\theta$ by finding the ML/MAP estimates:
\begin{eqnarray}
   \mathrm{E-step} &:& Q(\theta, \theta^{(k-1)}) =  E_{\theta^{(k-1)}}\big[l_c(\theta)|D, \theta^{(k-1)}\big] \\
   \mathrm{M-step} &:& \theta^{(k)} = \arg \max_{\theta}Q(\theta, \theta^{(k-1)}) + \log p(\theta)
\end{eqnarray}

\subsubsection{Gaussian Mixture Model (GMM)}

To derive the EM algorithm for GMM, we first need to compute the expected complete data log-likelihood:
\begin{eqnarray}
    Q(\theta, \theta^{(k-1)}) &=& E\bigg[\sum_i \log p(x_i, z_i|\theta) \bigg] \nonumber \\
    &=& \sum_i E\bigg[ \log \bigg[\prod_{k=1}^{K}(\pi_k p(x_i|\theta_k))^{1[z_i=k]}\bigg]\bigg] \\
    &=& \sum_i \sum_k E\big[1[z_i = k]\big] \log \big[\pi_k p(x_i|\theta_k) \big] \\
    &=& \sum_i \sum_k p(z_i = k|x_i,\theta^{(k-1)}) \log \big[\pi_k p(x_i|\theta_k) \big] \\
    &=& \sum_i \sum_k r_{ik} \log \pi_k + \sum_i \sum_k r_{ik} \log p(x_i|\theta_k)
\end{eqnarray}
where $r_{ik} = p(z_i = k | x_i, \theta^{(k-1)})$ is the soft assignment of point $x_i$ to cluster $k$.\\

\textbf{E-step}. Given $\theta^{(k-1)}$, we want to compute the soft assignments:
\begin{eqnarray}
    r_{ik} = p(z_i=k|x_i, \theta^{(k-1)}) = \frac{p(z_i=k,x_i|\theta^{(k-1)})}{\sum_{k=1}^{K}p(z_i=k,x_i|\theta^{(k-1)})} = \frac{p(x_i|z_i=k, \theta^{(k-1)})\pi_k}{\sum_{k=1}^{K}p(x_i|z_i=k, \theta^{(k-1)})\pi_k}
\end{eqnarray}
where $\pi_k = p(z_i = k)$ are the mixture proportions.\\

\textbf{M-step}. In the M step, we maximize $Q$ with respect to model parameters $\pi$ and $\theta_k$. First, let's find $\pi$ that maximizes the Lagrangian:
\begin{equation}
    \frac{\partial Q}{\partial \pi_k} = \frac{\partial}{\partial \pi_k} \bigg[\sum_i \sum_k r_{ik} \log \pi_k + \lambda(1 - \sum_k \pi_k)\bigg] = \sum_i r_{ik} \frac{1}{\pi_k} - \lambda = 0
\end{equation}
Substituting the above expression into the constraint, we get
\begin{equation}
    \sum_k \pi_k = 1 \implies  \sum_k \frac{1}{\lambda}\sum_i r_{ik} = 1 \implies \lambda = 
    \sum_i \sum_k r_{ik} = \sum_i 1 = N 
\end{equation}
Therefore, $\pi_k = \frac{1}{\lambda}\sum_i r_{ik} = \frac{1}{N}\sum_i r_{ik}$. To find the optimum parameters $\theta_k = \{\mu_k, \Sigma_k\}$, we want to optimize the terms of $Q$ that depend on $\theta_k$:
\begin{eqnarray}
    l(\mu_k,\Sigma_k) &=& \sum_i r_{ik} \log p(x_i|\theta_k) \nonumber \\
    &\propto& -\frac{1}{2} \sum_i r_{ik} \bigg[\log|\Sigma_k| + (x_i - \mu_k)^{T}\Sigma_{k}^{-1}(x_i - \mu_k) \bigg] 
\end{eqnarray}
To find the optimimum $\mu_k$, we differentiate the above expression. First, focusing on the second term inside the sum, we can make a substitution $y_i = x_i - \mu_k$: 
\begin{equation}
    \frac{\partial}{\partial \mu_k} (x_i - \mu_k)^{T}\Sigma_{k}^{-1}(x_i - \mu_k) = \frac{\partial}{\partial y_i} y_i^{T} \Sigma^{-1} y_i \frac{\partial y_i}{\partial \mu_k} = -1 \times (\Sigma^{-1} + \Sigma^{-T})y_i 
\end{equation}
Substituting the above expression, we get:
\begin{equation}
    \frac{\partial}{\partial \mu_k} l(\mu_k, \Sigma_k) \propto -\frac{1}{2}\sum_i r_{ik}\bigg[-2\Sigma^{-1}(x_i - \mu_k)\bigg] = \Sigma^{-1} \sum_i r_{ik} (x_i - \mu_k) = 0
\end{equation}
which implies that
\begin{equation}
    \sum_i r_{ik}(x_i - \mu_k) = 0 \implies \mu_k = \frac{\sum_i r_{ik} x_i}{\sum_i r_{ik}} 
\end{equation}
To compute optimum $\Sigma_k$, we can use the trace identity:
\begin{equation}
    x^{T}Ax = \mathrm{tr}(x^{T}Ax) = \mathrm{tr}(xx^{T}A) = \mathrm{tr}(Axx^{T})
\end{equation}
Using $\Lambda = \Sigma^{-1}$ notation, we have:
\begin{eqnarray}
    l(\Lambda) &\propto& -\frac{1}{2}\sum_i r_{ik} \log|\Lambda| - \frac{1}{2}\sum_i r_{ik} \mathrm{tr}\bigg[(x_i-\mu)(x_i-\mu)^{T}\Lambda\bigg] \nonumber \\ 
    &=& -\frac{1}{2}\sum_i r_{ik} \log|\Lambda| - \frac{1}{2}\mathrm{tr}(S_{\mu}\Lambda) 
\end{eqnarray}
Taking matrix derivative, we get:
\begin{eqnarray}
    \frac{\partial l(\Lambda)}{\partial \Lambda} &=& -\frac{1}{2}\sum_i r_{ik} \Lambda^{-T} - \frac{1}{2}S_{\mu}^{T} = 0 \nonumber \\
    \Lambda^{-1}\sum_i r_{ik} &=& S_{\mu}^{T} \implies \Sigma = \frac{S_{\mu}^{T}}{\sum_i r_{ik}} \nonumber \\
    \Sigma &=& \frac{\sum_i r_{ik} (x_i - \mu_k)(x_i - \mu_k)^{T}}{\sum_i r_{ik}} = \frac{\sum_i r_{ik}x_i x_{i}^{T}}{\sum_i r_{ik}} - \mu_k \mu_{k}^{T}
\end{eqnarray}
These equations make intuitive sense, the mean of cluster $k$ is a weighted by $r_{ik}$ average of all points assigned to cluster $k$, while the covariance is the weighted empirical scatter matrix. 

\subsubsection{Latent Dirichlet Allocation (LDA)}

The EM algorithm maximizes the log-likelihood of the observed data:
\begin{equation}
   \theta^{\ast} = \arg \max_{\theta} \sum_{i=1}^{n} \log p(x_i|\theta) = \arg \max_{\theta} \sum_{i=1}^{n} \log \big[\sum_z p(x_i,z_i|\theta) \big]
\end{equation}
where $p(x_i, z_i|\theta)$ is the joint distribution between the observed words $x_i$ and unobserved (latent) asssignments $z_i$ parameterized by $\theta$. In the case of LDA topic model, $\theta = \{\theta_d, \beta_{w|z}\}$ where $\theta_d \sim \mathrm{Dir}(\alpha)$ are document level topic proportions and $\beta_{w|z} \sim \mathrm{Dir}(\eta)$ are corpus level topics.\\

\textbf{E step}. Given $\theta^{(k)}$, we want to find soft counts $n(z)$ and $n(w,z)$. First, we compute the posterior over the topic assignments:
\begin{equation}
    p(z|w, \theta^{(k)}) = \frac{p(z,w|\theta^{(k)})}{\sum_z p(z,w|\theta^{(k)})} = \frac{\theta_{z|d}\beta_{w|z}}{\sum_z \theta_{z|d}\beta_{w|z}}
\end{equation}
Given the posterior $p(z|w,\theta^{(k)})$, we can compute the soft counts:
\begin{eqnarray}
    n(z)   &=& \sum_{i=1}^{n_d} p(z|w_i,\theta^{(k)}) \\
    n(w,z) &=& \sum_{d=1}^{D}\sum_{i=1}^{n_d} p(z|w_i,\theta^{(k)})\delta(w, w_i) 
\end{eqnarray}
where $\delta(w, w_i)$ is an indicator which is equal to $1$ when $w=w_i$ and $0$ otherwise.

\textbf{M step}. Given the soft counts $n(z)$ and $n(w,z)$, we want to update $\theta^{(k)}$:
\begin{eqnarray}
    \theta_{z|d}^{(k+1)} &=& \frac{n(z)}{n_d} \\
    \beta_{w|z}^{(k+1)} &=& \frac{n(w,z)}{\sum_{w \in V} n(w,z)}
\end{eqnarray}
where $n_d$ is the total number of words in document $d$ and $V$ is our vocabulary. Thus, we can initialize $\theta^{(0)}$ at random and iterate between the E-step and the M-step until convergence. The EM algorithm may require several restarts to avoid local optima. As a result of several iterations, the log-likelihood of observed words under the topic model will increase.    

\subsection{Gibbs Sampling}

%TODO: see David Mackay's book on general intro to different sampling techniques / insighful explanations / pseudo-code (e.g. HMC) etc.

\subsubsection{Latent Dirichlet Allocation}

The LDA graphical model associates each word $w_{i,d}$ with a topic label $z_{i,d} \in \{1,2,...,K\}$. Each document is associated with topic proportions $\theta_d$. The topics $\phi_k$ are shared across all documents. The hyper-parameters $\alpha$ and $\eta$ define our rior knowledge of topic proportions and topics, respectively. The full generative model can be summarized as follows:
\begin{eqnarray}
    \theta_d | \alpha &\sim& \mathrm{Dir}(\alpha)\\
    z_{i,d} | \theta_d &\sim& \mathrm{Cat}(\theta_d)\\
    \phi_k | \eta &\sim& \mathrm{Dir}(\eta)\\
    w_{i,d}|z_{i,d}=k,\phi &\sim& \mathrm{Cat}(\phi_k)
\end{eqnarray}

In order to derive Gibbs sampling updates, we focus on the joint distribution $p(w, z) = p(w|z)p(z)$ and we analytically integrate out $\theta$ and $\phi$ parameters. This is known as \textit{collapsed Gibbs sampling} and it tends to be more efficient because we are sampling in a lower dimensional space. The process of sampling $z$ and integrating out $\theta$ is known as \textit{Rao-Blackwellization} named after the following theorem:
\begin{theorem}
(Rao-Blackwell) Let $z$ and $\theta$ be dependent random variables and $f(z,\theta)$ be some scalar function, then:
\begin{equation}
    \mathrm{var}_{z,\theta}\big[f(z,\theta)\big] \geq \mathrm{var}_z \big[E_\theta [f(z,\theta) | z]\big]
\end{equation}
\end{theorem}
\textbf{Joint distribution $p(w,z)$.}\newline
We proceed with collapsed Gibbs sampling as follows:
\begin{eqnarray}
   p(w,z;\alpha,\eta) &=& p(w|z)p(z) = \int p(w,\phi|z)d\phi \times \int p(z,\theta)d\theta \nonumber \\ 
   &=& \int p(w|\phi,z)p(\phi)d\phi \times \int p(z|\theta)p(\theta)d\theta = I_1 \times I_2 
\end{eqnarray}
Let's compute $I_2$ first. We know that,
\begin{equation}
    p(\theta) = \prod_{d=1}^{D} p(\theta_d|\alpha) = \prod_{d=1}^{D} \mathrm{Dir}(\theta_d|\alpha) = \prod_{d=1}^{D}\frac{1}{Z(\alpha)}\prod_{k=1}^{K}\theta_{d,k}^{\alpha - 1},~~~\mathrm{where}~~~ Z(\alpha) = \frac{\prod \Gamma(\alpha_i)}{\Gamma(\sum \alpha_i)}
\end{equation}
where we assume that $\alpha_1 = \alpha_2 = \cdots = \alpha_k = \alpha$. Similarly, let's write out the distribution for $p(z|\theta)$:
\begin{eqnarray}
    p(z|\theta) &=& \prod_{d=1}^{D}\prod_{j=1}^{N_d}p(z_{d,j}|\theta_d) = \prod_{d=1}^{D}\prod_{j=1}^{N_d}\mathrm{Cat}(\theta_d) = \prod_{d=1}^{D}\prod_{j=1}^{N_d}\theta_{d,k}^{x_{k}^{j}} \nonumber \\ 
    &=& \prod_{d=1}^{D}\prod_{k=1}^{K}\theta_{d,k}^{\sum_{j=1}^{N_d}x_{k}^{j}} = \prod_{d=1}^{D}\prod_{k=1}^{K}\theta_{d,k}^{n_{d,k}} 
\end{eqnarray}
where $n_{d,k}$ is a count of words in document $d$ assigned to topic $k$, and $x_{k}^{j}$ is a one-hot encoded vector indicating the presence of topic $k$ assigned to word $j$. Combining the equations above, we get the following expression for $I_2$:
\begin{eqnarray}
p(z) &=& \int p(z|\theta) p(\theta) d\theta = \int \bigg[\prod_{d=1}^{D}\prod_{k=1}^{K}\theta_{d,k}^{n_{d,k}}\bigg] \bigg[\prod_{d=1}^{D}\frac{1}{Z(\alpha)}\prod_{k=1}^{K}\theta_{d,k}^{\alpha - 1}\bigg] d\theta \nonumber \\
&=& \frac{1}{Z(\alpha)}\prod_{d=1}^{D}\bigg[ \int \prod_{k=1}^{K}\theta_{d,k}^{\alpha + n_{d,k} - 1} d\theta_d \bigg] = \frac{1}{Z(\alpha)}\prod_{d=1}^{D}Z(\alpha + n_{d,k})
\end{eqnarray}
where we used the identity for the Dirichlet distribution:
\begin{equation}
    \int \prod_{i=1}^{k} x_{i}^{\alpha_i - 1} dx = Z(\alpha) = \frac{\prod_{i=1}^{n}\Gamma(\alpha_i)}{\Gamma(\sum_i \alpha_i)}
\end{equation}
Let's compute $I_1$ next.
\begin{equation}
    p(\phi) = \prod_{k=1}^{K}p(\phi_k|\eta) = \prod_{k=1}^{K}\mathrm{Dir}(\phi_k|\eta) = \prod_{k=1}^{K}\frac{1}{Z(\eta)}\prod_{v=1}^{V}\phi_{k,v}^{\eta - 1}, ~~~\mathrm{where}~~~Z(\eta) = \frac{\prod \Gamma(\eta_i)}{\Gamma(\sum \eta_i)}
\end{equation}
where we assume that $\eta_1 = \eta_2 = \cdots = \eta_v = \eta$. Similarly, let's write out the distribution for $p(w|\phi, z)$:
\begin{eqnarray}
    p(w|\phi, z) &=& \prod_{d=1}^{D}\prod_{j=1}^{Nd} \prod_{k=1}^{K}p(w_{d,j}|\phi_k) =  \prod_{d=1}^{D}\prod_{j=1}^{Nd} \prod_{k=1}^{K} \mathrm{Cat}(\phi_k) \nonumber \\
&=& \prod_{d=1}^{D}\prod_{j=1}^{Nd} \prod_{k=1}^{K} \prod_{v=1}^{V} \phi_{k,v}^{x_{v}^{d,j}} = \prod_{k=1}^{K} \prod_{v=1}^{V} \phi_{k,v}^{\sum_d \sum_j x_{v}^{d,j}} =  \prod_{k=1}^{K} \prod_{v=1}^{V} \phi_{k,v}^{n_{k,v}}
\end{eqnarray}
where $n_{k,v}$ is a count of the number of times word $v$ was assigned to topic $k$, and $x_{v}^{d,j}$ is a one-hot encoded vector indicating the presence of word $v$ in document $d$ at word location $j$. Combining the equations above, we get the following expression for $I_1$:
\begin{eqnarray}
    p(w|z) &=& \int p(w|\phi, z) p(\phi) d\phi = \int \bigg[\prod_{k=1}^{K} \prod_{v=1}^{V} \phi_{k,v}^{n_{k,v}}\bigg] \bigg[\prod_{k=1}^{K}\frac{1}{Z(\eta)}\prod_{v=1}^{V}\phi_{k,v}^{\eta - 1} \bigg] \nonumber \\
&=& \frac{1}{Z(\eta)}\prod_{k=1}^{K}\bigg[\int \prod_{v=1}^{V}\phi_{k,v}^{n_{k,v}+\eta - 1}\bigg] = \frac{1}{Z(\eta)}\prod_{k=1}^{K}Z(\eta + n_{k,v})
\end{eqnarray}
Finally, taking a product of $I_1$ and $I_2$, we can compute the joint distribution:
\begin{equation}
    p(w, z) = p(w|z)p(z) = \bigg[\frac{1}{Z(\eta)}\prod_{k=1}^{K}Z(\eta + n_{k,v})\bigg] \bigg[\frac{1}{Z(\alpha)}\prod_{d=1}^{D}Z(\alpha + n_{d,k})\bigg]
\end{equation}
\textbf{Posterior of $\theta$ and $\phi$.}\newline
We have integrated out $\theta$ and $\phi$ for collapsed Gibbs sampling, however, we still need to obtain the posterior distribution over these latent variables. In the derivation below, we condition on the assignments $z_{d,j}$ to compute the posterior distribution:
\begin{eqnarray}
    p(\theta_d|z_d) &\propto& p(z_d|\theta_d)p(\theta_d) = \prod_{j=1}^{N_d}p(z_{d,j}|\theta_d)\prod_{k=1}^{K}\theta_{d,k}^{\alpha - 1} = \prod_{k=1}^{K}\theta_{d,k}^{n_{d,k}}\prod_{k=1}^{K}\theta_{d,k}^{\alpha - 1} \nonumber \\
    &=& \prod_{k=1}^{K}\theta_{d,k}^{n_{d,k}+\alpha - 1} \sim \mathrm{Dir}(\alpha + n_{d,k})
\end{eqnarray}
Similarly, $\phi_{k,v} \sim \mathrm{Dir}(\eta + n_{k,v})$.\\\\
\textbf{Collapsed Gibbs updates.}\newline
We need to compute the full conditional distribution $p(z_i|z_{-i}, w)$. We can proceed as follows:
\begin{eqnarray}
    p(z_i=k|z_{-i}, w) &=& \frac{p(z_i, z_{-i}, w)}{p(z_{-i}, w)} = \frac{p(w|z)p(z)}{p(z_{-i}, w_{-i}, w_i)} = \frac{p(w|z)p(z)}{p(w_{-i}|z_{-i},w_i)p(z_{-i})p(w_i)} \nonumber \\
    &\propto& \frac{p(w|z)p(z)}{p(w_{-i}|z_{-i})p(z_{-i})} = \frac{\prod_{k=1}^{K}Z(\eta + n_{k,v})}{\prod_{k=1}^{K}Z(\eta + n_{-k,v})} \times \frac{\prod_{d=1}^{D}Z(\alpha + n_{d,k})}{\prod_{d=1}^{D}Z(\alpha + n_{d,-k})}
\end{eqnarray}
For all docs other than $d=d^{\prime}$, the numerator and denominator of the second factor are exactly the same:
\begin{eqnarray}
     \frac{\prod_{d=1}^{D}Z(\alpha + n_{d,k})}{\prod_{d=1}^{D}Z(\alpha + n_{d,-k})} &=& \frac{Z(\alpha + n_{d^{\prime}, k})}{Z(\alpha + n_{d^{\prime}, -k})} = \frac{\prod_{k=1}^{K}\Gamma(\alpha + n_{d^{\prime}, k})}{\prod_{k=1}^{K}\Gamma(\alpha + n_{d^{\prime}, -k})} \times \frac{\Gamma(\sum_{k=1}^{K}\alpha + n_{d^{\prime},-k})}{\Gamma(\sum_{k=1}^{K}\alpha + n_{d^{\prime}, k})} \nonumber \\\ &=& \frac{\Gamma(\alpha + n_{d^{\prime}, -k^{\prime}} + 1)}{\Gamma(\alpha + n_{d^{\prime}, -k^{\prime}})} \times \frac{\Gamma(\sum_{k=1}^{K}\alpha + n_{d^{\prime},-k})}{\Gamma(\sum_{k=1}^{K}\alpha + n_{d^{\prime}, -k} + 1)} \nonumber \\
   &=& \frac{\alpha + n_{d^{\prime}, -k^{\prime}}}{\sum_{k=1}^{K}\alpha + n_{d^{\prime},-k}}
\end{eqnarray}
where we used the identity: $\Gamma(x+1) = x! = x(x-1)! = x\Gamma(x)$. Similarly, we can simplify the first factor to:
\begin{equation}
    \frac{\prod_{k=1}^{K}Z(\eta + n_{k,v})}{\prod_{k=1}^{K}Z(\eta + n_{-k,v})} = \frac{\eta + n_{-k^{\prime}, v^{\prime}}}{\sum_{v=1}^{V}\eta + n_{-k^{\prime}, v}}
\end{equation}
Putting the two factors together, our full conditional distribution is:
\begin{equation}
    p(z_i = k^{\prime}|z_{-i}, w) \propto \bigg[ \frac{\eta + n_{-k^{\prime},v^{\prime}}}{\sum_{v=1}^{V}\eta + n_{-k^{\prime}, v}} \bigg] \bigg[\frac{\alpha + n_{d^{\prime}, -k^{\prime}}}{\sum_{k=1}^{K}\alpha + n_{d^{\prime},-k}} \bigg]
\end{equation}
In the expression above, the first ratio is a probability of word $w_i$ (for which we are sampling the label $z_i$) under topic $k^{\prime}$ and the second ratio is the probability of topic $k^{\prime}$ in document $d^{\prime}$. 

\subsection{Metropolis-Hastings (MH) Sampling}
\subsubsection{Gaussian Mixture Model (GMM)}

\subsection{Slice sampling}
\subsection{HMC sampling}


